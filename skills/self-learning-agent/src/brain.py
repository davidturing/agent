import os
import datetime
import requests
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
INSIGHTS_PATH = os.path.join(os.path.dirname(__file__), "../data/insights")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
MODEL_DEFAULT = os.getenv("MODEL_DEFAULT", "gemini-3-pro-preview")
GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_DEFAULT}:generateContent?key={GEMINI_API_KEY}"

def generate_daily_log(insights):
    date_str = datetime.date.today().isoformat()
    content = f"# David Agent Lite Log - {date_str}\n\n"
    
    content += "## ðŸ§  Insights (HTTP Mode)\n"
    for item in insights:
        content += f"- **{item.get('category', 'General')}**: {item.get('summary')}\n"
    
    content += "\n---\n*Generated by David Agent (Slim Architecture)*"
    
    os.makedirs(INSIGHTS_PATH, exist_ok=True)
    filename = os.path.join(INSIGHTS_PATH, f"daily_log_{date_str}_lite.md")
    
    with open(filename, "w") as f:
        f.write(content)
        
    return filename

def analyze_text(text):
    """Analyze text using Gemini API directly (no heavy library)."""
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": f"Summarize this technical update into a single bullet point insight: {text}"}]
        }]
    }
    
    try:
        resp = requests.post(GEMINI_URL, json=payload, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            # Extract text safely
            try:
                summary = data['candidates'][0]['content']['parts'][0]['text'].strip()
                return summary
            except (KeyError, IndexError):
                return "Analysis failed: Unexpected API response format."
        else:
            return f"Analysis failed: API Error {resp.status_code}"
    except Exception as e:
        return f"Analysis failed: {str(e)}"

def main():
    """Main execution point for pipeline integration."""
    print("Brain processing started...")
    
    insights = []
    
    # 1. Process GitHub Data
    github_raw = "skills/self-learning-agent/data/raw/github_events.json"
    if os.path.exists(github_raw):
        import json
        with open(github_raw, "r") as f:
            events = json.load(f)
            for event in events[:3]:
                summary = analyze_text(event.get('content', ''))
                insights.append({
                    "category": f"GitHub ({event.get('type')})",
                    "summary": summary
                })

    # 2. Process X (Twitter) Data
    x_raw = "skills/self-learning-agent/data/raw/x_tweets.json"
    if os.path.exists(x_raw):
        import json
        with open(x_raw, "r") as f:
            tweets = json.load(f)
            for tweet in tweets[:5]:
                author = tweet.get('user', {}).get('screen_name', 'Unknown')
                text = tweet.get('text', '')
                tags = tweet.get('account_tags', [])
                category = tweet.get('account_category', 'X Insight')
                persona = tweet.get('persona', 'Tech Guru')
                
                # Enhanced analysis prompt for X trends
                prompt = f"Analyze this tweet for the '{persona}' persona. Expert: @{author}, Focus: {tags}. Extract one high-value strategic insight: {text}"
                summary = analyze_text_with_prompt(prompt)
                
                insights.append({
                    "category": f"[{persona}] {category}",
                    "summary": f"@{author}: {summary}"
                })
    
    if insights:
        filename = generate_daily_log(insights)
        print(f"Brain: Generated {filename}")
    else:
        print("Brain: No raw data found to process.")

def analyze_text_with_prompt(prompt):
    """Helper for custom prompts."""
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }]
    }
    try:
        resp = requests.post(GEMINI_URL, json=payload, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            return data['candidates'][0]['content']['parts'][0]['text'].strip()
    except:
        pass
    return "Analysis failed."

if __name__ == "__main__":
    # Test run
    sample_text = "The system was rebooted to fix a memory leak in the redis container."
    print(analyze_text(sample_text))
